{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTKIXaproMc1",
    "outputId": "370164b0-fffa-41d8-887b-86d654e4d3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.12.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.12.0)\n",
      "Collecting tokenizers (from -r requirements.txt (line 5))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
      "Collecting keras-tuner (from -r requirements.txt (line 7))\n",
      "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (16.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.32.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner->-r requirements.txt (line 7)) (2.27.1)\n",
      "Collecting kt-legacy (from keras-tuner->-r requirements.txt (line 7))\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 3)) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner->-r requirements.txt (line 7)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner->-r requirements.txt (line 7)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 3)) (3.2.2)\n",
      "Installing collected packages: tokenizers, kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5 tokenizers-0.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "#installing depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "h16COAScIMhn"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.losses import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ygo4-Tz-JpaH"
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "lcp_train = pd.read_csv(\"lcp_single_train.tsv.txt\", sep=\"\\t\")\n",
    "lcp_test=pd.read_csv(\"lcp_single_test.tsv.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoWtwitkoxie"
   },
   "source": [
    "#Here im reloading the train dataset just to get construct the tokenizer, as I want the test test to have the same dictionary and padding as train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0D4a2W3UJw2s"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(lcp_train['sentence'])\n",
    "#loading tokenizer and fitting on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F1X55U-xJxfD"
   },
   "outputs": [],
   "source": [
    "#converting test sentences to sequence\n",
    "sent_seq = tokenizer.texts_to_sequences(lcp_train['sentence'])\n",
    "max_seq_len = max(len(seq) for seq in sent_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TFauPj75J4Wp"
   },
   "outputs": [],
   "source": [
    "#converting test sentences to sequences\n",
    "sent_seq_test = tokenizer.texts_to_sequences(lcp_test['sentence'])\n",
    "sent_seq_test = pad_sequences(sent_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VsgBKAbmKVwF"
   },
   "outputs": [],
   "source": [
    "#here converting the train words to sequence, just to get the max_length, which is used in the model training\n",
    "\n",
    "lcp_train.token=lcp_train.token.astype(str)\n",
    "token = tokenizer.texts_to_sequences(lcp_train['token'])\n",
    "max_word_len = max(len(w) for w in token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uaHgl93yKciT"
   },
   "outputs": [],
   "source": [
    "#converting test tokens into sequences\n",
    "lcp_test.token=lcp_test.token.astype(str)\n",
    "token_test = tokenizer.texts_to_sequences(lcp_test['token'])\n",
    "token_test = pad_sequences(token_test, maxlen=max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiSR9B78KsMV",
    "outputId": "6f3aca7e-6c05-4595-e5e0-2a81b455bffb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#converting it to one hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "corpus_encoded_test = onehot_encoder.fit_transform(lcp_test['corpus'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4PbLFImMK1mu"
   },
   "outputs": [],
   "source": [
    "#ref : https://www.tensorflow.org/guide/keras/save_and_serialize\n",
    "load_base_model = keras.models.load_model(\"Stack_RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk7YYDioLIAi",
    "outputId": "b0a5f049-437d-4011-e587-46aa07197572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 7s 204ms/step\n",
      "Mean Absolute Error(Without tuning): 0.11000688717584296\n"
     ]
    }
   ],
   "source": [
    "test_y=lcp_test['complexity'].values\n",
    "# Evaluate the model\n",
    "test_pred = load_base_model.predict([sent_seq_test, token_test, corpus_encoded_test])\n",
    "#print(test_pred,test_y)\n",
    "mae = np.mean(np.abs(test_pred - test_y))\n",
    "print(\"Mean Absolute Error(Without tuning):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTMXQQHboYFe",
    "outputId": "0a3c588b-e417-4a62-fc74-5f69113de97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 11s 315ms/step\n",
      "Mean Absolute Error(With tuning): 0.1072609143607685\n"
     ]
    }
   ],
   "source": [
    "load_tuned_model = keras.models.load_model(\"Tuned_Stack_RNN.h5\")\n",
    "# Evaluate the model\n",
    "test_pred = load_tuned_model.predict([sent_seq_test, token_test, corpus_encoded_test])\n",
    "#print(test_pred,test_y)\n",
    "mae = np.mean(np.abs(test_pred - test_y))\n",
    "print(\"Mean Absolute Error(With tuning):\", mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
