# -*- coding: utf-8 -*-
"""run_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MGn8a6kgdCbTLfjfKKqC2n3kxEA1JUGA

# Installing dependecies and loading data
"""

#!pip install -r requirements.txt
#installing depedencies

#importing libraries
import numpy as np
import pandas as pd 
import tensorflow
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Dense, Dropout, concatenate, BatchNormalization
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.losses import mean_absolute_error
from sklearn.preprocessing import OneHotEncoder
import keras_tuner as kt
from keras_tuner.tuners import RandomSearch

tensorflow.config.list_physical_devices('GPU')
#to configure GPU

#load the dataset
lcp_train = pd.read_csv("lcp_single_train.tsv.txt", sep="\t")
lcp_test=pd.read_csv("lcp_single_test.tsv.txt", sep="\t")

lcp_train.head()

lcp_train.describe()

lcp_test.head()

lcp_test.describe()

"""# Tokenization and Sequecing

Sentnece Vriable
"""

#here set the tokenizer limit to 10000, i.e., we will have a dictionary with top 10,000 words
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(lcp_train['sentence'])
sent_seq = tokenizer.texts_to_sequences(lcp_train['sentence'])
max_seq_len = max(len(seq) for seq in sent_seq)
sent_seq = pad_sequences(sent_seq, maxlen=max_seq_len) #padding the sequences to have uniform length

"""Token Variable"""

#getting sequeces for the tokens
lcp_train.token=lcp_train.token.astype(str)
token = tokenizer.texts_to_sequences(lcp_train['token'])
max_word_len = max(len(w) for w in token)
token = pad_sequences(token, maxlen=max_word_len) #padding the tokens to have uniform length

"""Corpus variable"""

#converting it to one hot encoding as we have 3 sources, bible,biomed and europarl
onehot_encoder = OneHotEncoder(sparse=False)
corpus_encoded = onehot_encoder.fit_transform(lcp_train['corpus'].values.reshape(-1, 1))

print(corpus_encoded)

"""The idea is to consider even corpus in input to the model.

#Splitting the data and Building a base stacked RNN model
"""

print('pre-procesing done')
print('Training Base Stacked RNN')

train_seq, val_seq, train_tok, val_tok, train_corpus, val_corpus, train_y, val_y = train_test_split(
    sent_seq, token, corpus_encoded, lcp_train['complexity'].values, test_size=0.2, random_state=42)
#splitting into train and validation set

#Initializing inputs
input_seq = Input(shape=(max_seq_len,)) #shape to max length of sentence sequence
input_token= Input(shape=(max_word_len,)) #shape to max length of tok sequence
input_corpus = Input(shape=(3,)) # as we have 3 values
#creating embeddings
x_sent_seq = Embedding(input_dim=10000, output_dim=128, input_length=max_seq_len)(input_seq)
x_tok = Embedding(input_dim=10000, output_dim=128, input_length=max_word_len)(input_token)
#defining stacked LSTM layes with droput and batchnormalization
x = LSTM(64, return_sequences=True)(x_sent_seq)
x = LSTM(32, return_sequences=True)(x)
x = LSTM(16)(x)
x = BatchNormalization()(x)
y = LSTM(64, return_sequences=True)(x_tok)
y = LSTM(32, return_sequences=True)(y)
y = LSTM(16)(y)
y = BatchNormalization()(y)
#merging the outputs from layers
merged = concatenate([x, y, input_corpus])
x = Dense(64, activation='relu')(merged)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(32, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
#finally getting one output
output = Dense(1, activation='sigmoid')(x)
#building the model
model = Model(inputs=[input_seq, input_token, input_corpus], outputs=output)

model.summary()

from tensorflow.keras.utils import  plot_model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
plot_model(model,show_shapes=True)

model.compile(loss=mean_absolute_error, optimizer='adam',metrics='accuracy')

history = model.fit([train_seq, train_tok, train_corpus], train_y, epochs=10, batch_size=32, validation_data=([val_seq, val_tok, val_corpus], val_y))
model.save("Stack_RNN.h5")

print(history.history.keys())

# Commented out IPython magic to ensure Python compatibility.
# Plot the loss curve with best hp and val loss
import matplotlib.pyplot as plt
# %matplotlib inline
epochs=range(10)
train_acc=history.history['loss']
valid_acc = history.history['val_loss']
plt.plot(epochs,train_acc,'bo',label='Training loss')
plt.plot(epochs,valid_acc,'r',label='Validation loss')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

"""# Tuning the stacked RNN model"""

print('Base model trained')
print('Tuning Hyper parameters using keras tuner')

#ref : https://keras.io/keras_tuner/
#fuction to tune the hyper parameters - selected three things to tune 1.Optimizer 2. learning rate 3.Dropout Rate
def tuning(hp):
  input_seq = Input(shape=(max_seq_len,)) 
  input_token= Input(shape=(max_word_len,))
  input_corpus = Input(shape=(3,)) # as we have 3 values

  x_sent_seq = Embedding(input_dim=10000, output_dim=128, input_length=max_seq_len)(input_seq)
  x_tok = Embedding(input_dim=10000, output_dim=128, input_length=max_word_len)(input_token)

  x = LSTM(64, return_sequences=True)(x_sent_seq)
  x = LSTM(32, return_sequences=True)(x)
  x = LSTM(16)(x)
  x = BatchNormalization()(x)

  y = LSTM(64, return_sequences=True)(x_tok)
  y = LSTM(32, return_sequences=True)(y)
  y = LSTM(16)(y)
  y = BatchNormalization()(y)

  merged = concatenate([x, y, input_corpus])

  x = Dense(64, activation='relu')(merged)
  x = BatchNormalization()(x)
  x = Dropout(hp.Choice(name='dropout',values=[0.2,0.5,0.7]))(x)

  x = Dense(32, activation='relu')(x)
  x = BatchNormalization()(x)
  x = Dropout(hp.Choice(name='dropout',values=[0.2,0.5,0.7]))(x)

  output = Dense(1, activation='sigmoid')(x)

  model = Model(inputs=[input_seq, input_token, input_corpus], outputs=output)

  
  optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop'])
  if optimizer == 'adam':
    optimizer = tensorflow.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))
  else:
    optimizer = tensorflow.keras.optimizers.RMSprop(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))

  model.compile(optimizer=optimizer,
                  loss='mean_absolute_error',
                  metrics=['accuracy'])
  return model

tuner = kt.RandomSearch(
    tuning,
    objective='val_loss')

stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
# Perform hypertuning
tuner.search([train_seq, train_tok, train_corpus], train_y, epochs=30,batch_size=64,validation_data=([val_seq, val_tok, val_corpus], val_y),callbacks=[stop_early])
best_hp=tuner.get_best_hyperparameters()[0]

print('Best Learning Rate: ',best_hp.get('learning_rate'))
print('Best Optimizer :',best_hp.get('optimizer'))
print('Best Dropout Rate: ',best_hp.get('dropout'))

"""# Re-training on the whole training set with tuned parameters"""

print('Retraining using whole training dataset with best params')

model.compile(loss=mean_absolute_error, optimizer=tensorflow.optimizers.RMSprop(learning_rate=0.01),metrics='accuracy')

history = model.fit([sent_seq, token, corpus_encoded], lcp_train['complexity'].values, epochs=30, batch_size=32)
model.save("Tuned_Stack_RNN.h5")

print('Tuned model trained')