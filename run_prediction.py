# -*- coding: utf-8 -*-
"""run_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWL1qSfRos8FkkZK4AH73YiXC4eFADTW
"""

#pip install -r "requirements.txt"
#installing depedencies

#importing libraries
import numpy as np
import pandas as pd 
import tensorflow
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.losses import mean_absolute_error
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras

#load the dataset
lcp_train = pd.read_csv("lcp_single_train.tsv.txt", sep="\t")
lcp_test=pd.read_csv("lcp_single_test.tsv.txt", sep="\t")

"""#Here im reloading the train dataset just to get construct the tokenizer, as I want the test test to have the same dictionary and padding as train dataset."""

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(lcp_train['sentence'])
#loading tokenizer and fitting on train dataset

#converting test sentences to sequence
sent_seq = tokenizer.texts_to_sequences(lcp_train['sentence'])
max_seq_len = max(len(seq) for seq in sent_seq)

#converting test sentences to sequences
sent_seq_test = tokenizer.texts_to_sequences(lcp_test['sentence'])
sent_seq_test = pad_sequences(sent_seq_test, maxlen=max_seq_len)

#here converting the train words to sequence, just to get the max_length, which is used in the model training

lcp_train.token=lcp_train.token.astype(str)
token = tokenizer.texts_to_sequences(lcp_train['token'])
max_word_len = max(len(w) for w in token)

#converting test tokens into sequences
lcp_test.token=lcp_test.token.astype(str)
token_test = tokenizer.texts_to_sequences(lcp_test['token'])
token_test = pad_sequences(token_test, maxlen=max_word_len)

#converting it to one hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
corpus_encoded_test = onehot_encoder.fit_transform(lcp_test['corpus'].values.reshape(-1, 1))

#ref : https://www.tensorflow.org/guide/keras/save_and_serialize
load_base_model = keras.models.load_model("Stack_RNN.h5")

test_y=lcp_test['complexity'].values
# Evaluate the model
test_pred = load_base_model.predict([sent_seq_test, token_test, corpus_encoded_test])
#print(test_pred,test_y)
mae = np.mean(np.abs(test_pred - test_y))
print("Mean Absolute Error(Without tuning):", mae)

load_tuned_model = keras.models.load_model("Tuned_Stack_RNN.h5")
# Evaluate the model
test_pred = load_tuned_model.predict([sent_seq_test, token_test, corpus_encoded_test])
#print(test_pred,test_y)
mae = np.mean(np.abs(test_pred - test_y))
print("Mean Absolute Error(With tuning):", mae)